{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "PUBDIR = os.getcwd()\n",
    "ROOT_DIR = os.path.dirname(PUBDIR)\n",
    "DATA_DIR = os.path.join(ROOT_DIR, \"data\")\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ontology\n",
    "from core.data_model import Patient, Disease, Ontology\n",
    "from core.data_model import Diseases\n",
    "from core.io_ops import load_pickle\n",
    "\n",
    "disease_data = load_pickle(os.path.join(DATA_DIR, \"diseases.pickle\"))\n",
    "vectorized_hpo = load_pickle(os.path.join(DATA_DIR, \"hpo_definition.vector.pickle\"))\n",
    "ontology = Ontology(vectorized_hpo)\n",
    "omim_diseases = Diseases([disease for disease in disease_data if disease.id.startswith(\"OMIM\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load benchmark patient dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download public dataset from zeonodo\n",
    "if not os.path.exists('../data/phenopackets/'):\n",
    "    ! wget https://zenodo.org/records/3905420/files/phenopackets.zip?download=1\n",
    "    ! mv phenopackets.zip?download=1 ../data/phenopackets.zip\n",
    "    ! unzip -o ../data/phenopackets.zip -d ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from core.benchmark import load_phenopacket_patients\n",
    "from core.data_model import HPO, HPOs, Patient, Patients\n",
    "\n",
    "benchmark_patients:Patients = load_phenopacket_patients(\n",
    "    phenopacket_dir=os.path.join(DATA_DIR, \"phenopackets\"),\n",
    "    ontology=ontology\n",
    ")\n",
    "print(benchmark_patients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Phen2Disease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download prerequsite file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../data/lin_similarity_matrix.json\"):\n",
    "    ! pip install gdown\n",
    "    ! gdown 1CSYfDj5fG9SsosIDlG-hLAoKp9eMHxjH\n",
    "    ! gunzip -f lin_similarity_matrix.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "from core.benchmark import get_phen2disease\n",
    "from core.io_ops import read_json\n",
    "\n",
    "if not os.path.exists(\"phen2disease_result.npy\"):\n",
    "    phen2disease_result = np.zeros((len(benchmark_patients), len(omim_diseases)))\n",
    "    labels = np.zeros((len(benchmark_patients), len(omim_diseases)))\n",
    "    pheno2disease_sim_mat:dict = read_json(\"../data/lin_similarity_matrix.json\")\n",
    "        \n",
    "    for patient_idx, patient in tqdm.tqdm(enumerate(benchmark_patients), total=len(benchmark_patients)):\n",
    "        true_disease_indices = [\n",
    "            disease_idx for disease_idx, disease \n",
    "            in enumerate(omim_diseases) \n",
    "            if disease.id in patient.disease_ids\n",
    "        ]\n",
    "        labels[patient_idx, true_disease_indices] = 1\n",
    "        \n",
    "        process = list()\n",
    "        for disease_idx, disease in enumerate(omim_diseases):\n",
    "            phen2disease_result[patient_idx, disease_idx] = get_phen2disease(patient, disease, pheno2disease_sim_mat)\n",
    "\n",
    "    np.save(\"phen2disease_result\", phen2disease_result)\n",
    "    \n",
    "else:\n",
    "    phen2disease_result = np.load(\"phen2disease_result.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: LaRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import mlflow\n",
    "\n",
    "from core.datasets import (\n",
    "    StochasticPairwiseDataset,\n",
    "    collate_for_stochastic_pairwise_eval,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from core.networks import Transformer\n",
    "from mlflow_settings import TRACKING_URI\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "\n",
    "model_url = \"mlflow-artifacts:/25/8764a3fa4a4f46cbb20c33fc92bccb9f/artifacts/model\"\n",
    "lara = mlflow.pytorch.load_model(model_url).to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build dataset for LaRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def cosine_sim(vector1, vector2):\n",
    "    return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "\n",
    "lara_disease_vectors = list()\n",
    "with torch.no_grad():\n",
    "    for disease in tqdm.tqdm(omim_diseases):\n",
    "        disease_tensor:torch.Tensor = torch.tensor(\n",
    "            disease.hpos.vector, \n",
    "            dtype=torch.float32, \n",
    "            device=DEVICE\n",
    "        ).unsqueeze(dim=0)\n",
    "        target_vector = lara(disease_tensor).squeeze().cpu().numpy()\n",
    "        lara_disease_vectors.append(target_vector)\n",
    "        \n",
    "lara_disease_vectors = np.stack(lara_disease_vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"lara_result.npy\"):\n",
    "    lara_result = np.zeros((len(benchmark_patients), len(omim_diseases)))\n",
    "    with torch.no_grad():\n",
    "        for patient_idx, patient in tqdm.tqdm(enumerate(benchmark_patients)):\n",
    "            input_src = torch.tensor(\n",
    "                    patient.hpos.vector, dtype=torch.float32, device=DEVICE\n",
    "            ).unsqueeze(dim=0)\n",
    "            patient_vector = lara(input_src).squeeze().cpu().numpy()\n",
    "                \n",
    "            for disease_idx, disease in enumerate(omim_diseases):\n",
    "                lara_result[patient_idx, disease_idx] = cosine_sim(\n",
    "                    patient_vector,\n",
    "                    lara_disease_vectors[disease_idx]\n",
    "                )\n",
    "    np.save(\"lara_result\", lara_result)\n",
    "    \n",
    "else:\n",
    "    lara_result = np.load(\"lara_result.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Node level Semantic similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install SemanticSimilarity (Node level similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../SemanticSimilarity'):\n",
    "    ! git clone https://github.com/4pygmalion/SemanticSimilarity.git\n",
    "    ! cd SemanticSimilarity\n",
    "    ! python3 -m pip install . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"base_result.npy\"):\n",
    "\n",
    "    from SemanticSimilarity.calculator import NodeLevelSimilarityCalculator\n",
    "    from SemanticSimilarity.data_model import Phenotype\n",
    "    import tqdm\n",
    "\n",
    "    from omegaconf import OmegaConf\n",
    "    conf = OmegaConf.load(\"/data1/benny_dev/symptom_similarity/SemanticSimilarity/config.yaml\")\n",
    "    tb_cal = NodeLevelSimilarityCalculator(conf)\n",
    "    tb_cal.set_level()\n",
    "    tb_cal.set_mica_mat()\n",
    "\n",
    "    def calculate_score(p, d):\n",
    "        node_level = {}\n",
    "        node_level[p.id] = {}\n",
    "        p_syms = {Phenotype(id_, name) for id_, name in zip(p.hpos.id2hpo.keys(), p.hpos.name2hpo.keys())}\n",
    "        d_syms = {Phenotype(id_, name) for id_, name in zip(d.hpos.id2hpo.keys(), d.hpos.name2hpo.keys())}\n",
    "        score = tb_cal.get_semantic_similarity(p_syms, d_syms)\n",
    "        return score\n",
    "\n",
    "    base_result = np.zeros((len(benchmark_patients), len(omim_diseases)))\n",
    "    for patient_idx, patient in tqdm.tqdm(enumerate(benchmark_patients)):\n",
    "        for disease_idx, disease in enumerate(omim_diseases):  ## 필요시 병렬처리\n",
    "            score = calculate_score(patient, disease)\n",
    "            base_result[patient_idx, disease_idx] = score\n",
    "\n",
    "    np.save(\"base_result\", base_result)\n",
    "    \n",
    "else:\n",
    "    base_result = np.load(\"base_result.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 비교 평가\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 np.ndarray을 이용하여 계산\n",
    "- phen2disease_result(np.ndarray). shape=(# of patients, # of omim disease)\n",
    "- lara_result(np.ndarray). shape=(# of patients, # of omim disease)\n",
    "- base_result(np.ndarray). shape=(# of patients, # of omim disease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from core_3asc.metric import topk_recall\n",
    "\n",
    "results = []\n",
    "for patient_idx, patient in tqdm(enumerate(benchmark_patients)):\n",
    "\n",
    "    label = np.zeros((len(omim_diseases), ))\n",
    "    scores_base = np.zeros((len(omim_diseases), ))\n",
    "    scores_pd = np.zeros((len(omim_diseases), ))\n",
    "    scores_model = np.zeros((len(omim_diseases), ))\n",
    "\n",
    "    for disease_idx, disease in enumerate(omim_diseases):\n",
    "        if disease.id in patient.disease_ids:\n",
    "            label[disease_idx] = 1\n",
    "        \n",
    "        scores_base = base_result[patient_idx]\n",
    "        scores_pd = phen2disease_result[patient_idx]\n",
    "        scores_lara = lara_result[patient_idx]\n",
    "\n",
    "    results.append({\n",
    "        \"p_id\": patient.id, \n",
    "        \"scores_base\": scores_base, \n",
    "        \"scores_pd\": scores_pd, \n",
    "        \"scores_model\": scores_lara, \n",
    "\n",
    "        \"top_1_base\": topk_recall(scores_base, label, k=1),\n",
    "        \"top_1_pd\": topk_recall(scores_pd, label, k=1),\n",
    "        \"top_1_model\": topk_recall(scores_lara, label, k=1),\n",
    "        \n",
    "        \"top_5_base\": topk_recall(scores_base, label, k=5),\n",
    "        \"top_5_pd\": topk_recall(scores_pd, label, k=5),\n",
    "        \"top_5_model\": topk_recall(scores_lara, label, k=5),\n",
    "\n",
    "        \"top_10_base\": topk_recall(scores_base, label, k=10),\n",
    "        \"top_10_pd\": topk_recall(scores_pd, label, k=10),\n",
    "        \"top_10_model\": topk_recall(scores_lara, label, k=10),\n",
    "\n",
    "        \"top_15_base\": topk_recall(scores_base, label, k=15),\n",
    "        \"top_15_pd\": topk_recall(scores_pd, label, k=15),\n",
    "        \"top_15_model\": topk_recall(scores_lara, label, k=15),\n",
    "\n",
    "        \"top_20_base\": topk_recall(scores_base, label, k=20),\n",
    "        \"top_20_pd\": topk_recall(scores_pd, label, k=20),\n",
    "        \"top_20_model\": topk_recall(scores_lara, label, k=20),\n",
    "\n",
    "        \"top_30_base\": topk_recall(scores_base, label, k=30),\n",
    "        \"top_30_pd\": topk_recall(scores_pd, label, k=30),\n",
    "        \"top_30_model\": topk_recall(scores_lara, label, k=30),\n",
    "\n",
    "        \"top_40_base\": topk_recall(scores_base, label, k=40),\n",
    "        \"top_40_pd\": topk_recall(scores_pd, label, k=40),\n",
    "        \"top_40_model\": topk_recall(scores_lara, label, k=40),\n",
    "\n",
    "        \"top_50_base\": topk_recall(scores_base, label, k=50),\n",
    "        \"top_50_pd\": topk_recall(scores_pd, label, k=50),\n",
    "        \"top_50_model\": topk_recall(scores_lara, label, k=50),\n",
    "\n",
    "        \"top_75_base\": topk_recall(scores_base, label, k=75),\n",
    "        \"top_75_pd\": topk_recall(scores_pd, label, k=75),\n",
    "        \"top_75_model\": topk_recall(scores_lara, label, k=75),\n",
    "\n",
    "        \"top_100_base\": topk_recall(scores_base, label, k=100),\n",
    "        \"top_100_pd\": topk_recall(scores_pd, label, k=100),\n",
    "        \"top_100_model\": topk_recall(scores_lara, label, k=100),\n",
    "        \n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result_df = pd.DataFrame(results)\n",
    "result_df = result_df.set_index(\"p_id\")\n",
    "result_df = result_df[list(set(result_df.columns) - {'p_id', 'scores_base', 'scores_pd', 'scores_model'})]\n",
    "\n",
    "data = (result_df.sum(0) / len(result_df)).to_dict()\n",
    "\n",
    "# Initialize empty dictionaries for 'base', 'pd', and 'model' data\n",
    "base_data = {f\"top{i}\": None for i in [1,5,10,15,20,30,40,50,75,100]}\n",
    "pd_data = {f\"top{i}\": None for i in [1,5,10,15,20,30,40,50,75,100]}\n",
    "model_data = {f\"top{i}\": None for i in [1,5,10,15,20,30,40,50,75,100]}\n",
    "\n",
    "# Organize the data according to 'base', 'pd', and 'model' categories\n",
    "for key, value in data.items():\n",
    "    if 'base' in key:\n",
    "        base_data[f\"top{key.split('_')[1]}\"] = value\n",
    "    elif 'pd' in key:\n",
    "        pd_data[f\"top{key.split('_')[1]}\"] = value\n",
    "    elif 'model' in key:\n",
    "        model_data[f\"top{key.split('_')[1]}\"] = value\n",
    "\n",
    "# Create a DataFrame with 'base', 'pd', and 'model' as rows and 'top1', 'top10', 'top50', 'top100' as columns\n",
    "df = pd.DataFrame([base_data, pd_data, model_data], index=['baseline', 'Pheno2Disease', 'LLM-based'])\n",
    "df.index.name = 'Method'\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 3\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "fig_df = pd.DataFrame([base_data, pd_data, model_data], index=['Resnik-based IC', 'Pheno2Disease', 'LaRa'])\n",
    "fig_df.index.name = 'Method'\n",
    "\n",
    "\n",
    "# Example DataFrame\n",
    "# Plotting the recall curve\n",
    "plt.figure(figsize=(6, 6))\n",
    "for index, row in fig_df.iterrows():\n",
    "    plt.plot(list(row.index), list(row.values), marker='o', label=index)\n",
    "\n",
    "plt.title('Real world dataset: rare disease patient data')\n",
    "plt.xlabel('Top-k')\n",
    "plt.ylabel('Top-k Recall')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
