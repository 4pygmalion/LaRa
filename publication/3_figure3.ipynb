{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "PUBDIR = os.getcwd()\n",
    "ROOT_DIR = os.path.dirname(PUBDIR)\n",
    "DATA_DIR = os.path.join(ROOT_DIR, \"data\")\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ontology\n",
    "from core.data_model import Patient, Disease, Ontology\n",
    "from core.data_model import Diseases\n",
    "from core.io_ops import load_pickle\n",
    "\n",
    "disease_data = load_pickle(os.path.join(DATA_DIR, \"diseases.pickle\"))\n",
    "vectorized_hpo = load_pickle(os.path.join(DATA_DIR, \"hpo_definition.vector.pickle\"))\n",
    "ontology = Ontology(vectorized_hpo)\n",
    "omim_diseases = Diseases([disease for disease in disease_data if disease.id.startswith(\"OMIM\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load benchmark patient dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download public dataset from zeonodo\n",
    "if not os.path.exists('../data/phenopackets.zip'):\n",
    "    ! wget https://zenodo.org/records/3905420/files/phenopackets.zip?download=1\n",
    "    ! mv phenopackets.zip?download=1 ../data/phenopackets.zip\n",
    "    ! unzip -o ../data/phenopackets.zip -d ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from core.benchmark import load_phenopacket_patients\n",
    "from core.data_model import HPO, HPOs, Patient, Patients\n",
    "\n",
    "benchmark_patients:Patients = load_phenopacket_patients(\n",
    "    phenopacket_dir=os.path.join(DATA_DIR, \"phenopackets\"),\n",
    "    ontology=ontology\n",
    ")\n",
    "print(benchmark_patients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1: Phen2Disease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download prerequsite file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"lin_similarity_matrix.json\"):\n",
    "    ! pip install gdown\n",
    "    ! gdown 1CSYfDj5fG9SsosIDlG-hLAoKp9eMHxjH\n",
    "    ! gunzip -f lin_similarity_matrix.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "from core.benchmark import get_phen2disease\n",
    "from core.io_ops import read_json\n",
    "\n",
    "phen2disease_result = np.zeros((len(benchmark_patients), len(omim_diseases)))\n",
    "labels = np.zeros((len(benchmark_patients), len(omim_diseases)))\n",
    "pheno2disease_sim_mat:dict = read_json(\"lin_similarity_matrix.json\")\n",
    "\n",
    "for patient_idx, patient in tqdm.tqdm(enumerate(benchmark_patients), total=len(benchmark_patients)):\n",
    "    true_disease_indices = [\n",
    "        disease_idx for disease_idx, disease \n",
    "        in enumerate(omim_diseases) \n",
    "        if disease.id in patient.disease_ids\n",
    "    ]\n",
    "    labels[patient_idx, true_disease_indices] = 1\n",
    "    \n",
    "    for disease_idx, disease in enumerate(omim_diseases):\n",
    "        phen2disease_result[patient_idx, disease_idx] = get_phen2disease(patient, disease, pheno2disease_sim_mat)\n",
    "\n",
    "\n",
    "phen2disease_result[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model2: LaRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/heon/repositories/LaRa/data/val_top100_0.584.ckpt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnetworks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Transformer\n\u001b[1;32m      9\u001b[0m best_model \u001b[38;5;241m=\u001b[39m Transformer(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m128\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2048\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_first\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     16\u001b[0m })\n\u001b[1;32m     17\u001b[0m best_model\u001b[38;5;241m.\u001b[39mload_state_dict(\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mROOT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval_top100_0.584.ckpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda:3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m best_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     24\u001b[0m best_model \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mcuda(\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/lara/lib/python3.8/site-packages/torch/serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/envs/lara/lib/python3.8/site-packages/torch/serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/envs/lara/lib/python3.8/site-packages/torch/serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/heon/repositories/LaRa/data/val_top100_0.584.ckpt'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from core.datasets import (\n",
    "    StochasticPairwiseDataset,\n",
    "    collate_for_stochastic_pairwise_eval,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from core.networks import Transformer\n",
    "\n",
    "best_model = Transformer(**{\n",
    "    \"output_size\": 128,\n",
    "    \"hidden_dim\": 2048,\n",
    "    \"input_size\": 1536,\n",
    "    \"n_layers\": 32,\n",
    "    \"nhead\": 32,\n",
    "    \"batch_first\": False,\n",
    "})\n",
    "best_model.load_state_dict(\n",
    "    torch.load(\n",
    "        os.path.join(ROOT_DIR, \"data\", \"val_top100_0.584.ckpt\"),\n",
    "        map_location='cuda:3'\n",
    "    )\n",
    ")\n",
    "best_model.eval()\n",
    "best_model.cuda(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.augmentation import TruncateOrPad\n",
    "\n",
    "benchmark_dataset = StochasticPairwiseDataset(\n",
    "    benchmark_patients,\n",
    "    disease_data,\n",
    "    max_len=15,\n",
    ")\n",
    "benchmark_dataset.validate()\n",
    "\n",
    "benchmark_dataloader = DataLoader(\n",
    "    benchmark_dataset,\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_for_stochastic_pairwise_eval,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "padder = TruncateOrPad(15, stochastic=False, weighted_sampling=True)\n",
    "def get_score_from_model(patient, disease):\n",
    "    with torch.no_grad():\n",
    "        input_src = padder(\n",
    "            torch.tensor(patient.hpos.vector, dtype=torch.float32, device=DEVICE), patient\n",
    "        )\n",
    "        target_src = padder(\n",
    "            torch.tensor(disease.hpos.vector, dtype=torch.float32, device=DEVICE), disease\n",
    "        )\n",
    "\n",
    "        input_vector = best_model(input_src)\n",
    "        target_vector = best_model(target_src)\n",
    "\n",
    "        scores = (\n",
    "            torch.nn.functional.cosine_similarity(\n",
    "                input_vector, target_vector\n",
    "            )\n",
    "            .squeeze(-1)\n",
    "            .detach()\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "\n",
    "        del input_src\n",
    "        del input_vector\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return scores\n",
    "    \n",
    "lara_result = np.zeros_like(phen2disease_result)\n",
    "for patient_idx, paitent in tqdm.tqdm(enumerate(benchmark_patients)):\n",
    "    for disease_idx, disease in enumerate(omim_diseases):\n",
    "        lara_result[patient_idx, disease_idx] = get_score_from_model(paitent, disease)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cache에서 가져오기 모든 데이터: benchmark +disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.datasets import StochasticPairwiseDataset, collate_for_stochastic_pairwise_eval\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# disease cache\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from core_3asc.metric import topk_recall\n",
    "\n",
    "cached_vector = {}\n",
    "whole_disease = benchmark_dataset.disease_tensors\n",
    "with torch.no_grad():\n",
    "    for disease_id, tensor in tqdm(whole_disease.items()):\n",
    "        cached_vector[disease_id] = best_model(tensor.cuda(3)).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(benchmark_dataset)\n",
    "print(benchmark_dataloader)\n",
    "p_samples = np.random.choice(range(len(benchmark_dataloader)), 300).tolist()\n",
    "print(p_samples)\n",
    "print(disease_data[:])\n",
    "for i in benchmark_dataset:\n",
    "    print(i)\n",
    "    break\n",
    "\n",
    "for i in omim_diseases:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LaRa 가져오기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 사내 증상유사도 모델 가져오기 + 계산수식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from SemanticSimilarity.calculator import NodeLevelSimilarityCalculator\n",
    "\n",
    "conf = OmegaConf.load(\"/data1/benny_dev/symptom_similarity/SemanticSimilarity/config.yaml\")\n",
    "# 원본 알고리즘 이용한 계산\n",
    "tb_cal = NodeLevelSimilarityCalculator(conf)\n",
    "tb_cal.set_level()\n",
    "tb_cal.set_mica_mat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from SemanticSimilarity.data_model import Phenotype\n",
    "\n",
    "def calculate_score(p, d):\n",
    "    node_level = {}\n",
    "    node_level[p.id] = {}\n",
    "    p_syms = {Phenotype(id_, name) for id_, name in zip(p.hpos.id2hpo.keys(), p.hpos.name2hpo.keys())}\n",
    "    d_syms = {Phenotype(id_, name) for id_, name in zip(d.hpos.id2hpo.keys(), d.hpos.name2hpo.keys())}\n",
    "    score = tb_cal.get_semantic_similarity(p_syms, d_syms)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phen2Disease 유사도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(os.path.join(DATA_DIR, \"tyler_backup/lin_similarity_matrix.json\"), \"r\") as f:\n",
    "    similarity_matrix = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LaRa 계산 수식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "padder = TruncateOrPad(\n",
    "            15, stochastic=False, weighted_sampling=True\n",
    "        )\n",
    "\n",
    "def get_score_from_model(patient, disease):\n",
    "    with torch.no_grad():\n",
    "        input_src = padder(\n",
    "            torch.tensor(patient.hpos.vector, dtype=torch.float32, device=\"cuda:3\"), patient\n",
    "        )\n",
    "        target_src = padder(\n",
    "            torch.tensor(disease.hpos.vector, dtype=torch.float32, device=\"cuda:3\"), disease\n",
    "        )\n",
    "\n",
    "        input_vector = best_model(input_src)\n",
    "        target_vector = best_model(target_src)\n",
    "        # target_vector = cached_vector[disease.id]\n",
    "\n",
    "        scores = (\n",
    "            torch.nn.functional.cosine_similarity(\n",
    "                input_vector, target_vector\n",
    "            )\n",
    "            .squeeze(-1)\n",
    "            .detach()\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "\n",
    "        del input_src\n",
    "        del input_vector\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return scores\n",
    "    \n",
    "\n",
    "def get_att_weight(disease):\n",
    "    with torch.no_grad():\n",
    "        input_src = padder(\n",
    "            torch.tensor(disease.hpos.vector, dtype=torch.float32).cuda(3), disease\n",
    "        )\n",
    "        return best_model.get_att_weight(input_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in disease_data_sample:\n",
    "    print(i.vector)\n",
    "    break\n",
    "\n",
    "# for i in benchmark_patients:\n",
    "#     print(i)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 비교 평가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from core_3asc.metric import topk_recall\n",
    "\n",
    "disease_data_sample = [i for i in omim_diseases]\n",
    "patient_data_sample = [i for i in benchmark_patients]\n",
    "\n",
    "result = []\n",
    "for p in tqdm(patient_data_sample):\n",
    "\n",
    "    label = np.zeros((len(disease_data_sample), ))\n",
    "    scores_base = np.zeros((len(disease_data_sample)),)\n",
    "    scores_pd = np.zeros((len(disease_data_sample)), )\n",
    "    scores_model = np.zeros((len(disease_data_sample)), )\n",
    "\n",
    "    # p_vector = best_model(\n",
    "    #     torch.from_numpy(patient.hpos.vector).cuda(3).float()\n",
    "    # ).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "    for i, d in enumerate(disease_data_sample):\n",
    "        if d.id in p.disease_ids:\n",
    "            label[i] = 1\n",
    "\n",
    "        scores_base[i] = calculate_score(p, d)\n",
    "        scores_pd[i] = get_pheno2disease(p, d)\n",
    "        scores_model[i] = get_score_from_model(p, d)\n",
    "    \n",
    "    result.append({\n",
    "        \"p_id\": p.id, \n",
    "        \"scores_base\": scores_base, \n",
    "        \"scores_pd\": scores_pd, \n",
    "        \"scores_model\": scores_model, \n",
    "\n",
    "        \"top_1_base\": topk_recall(scores_base, label, k=1),\n",
    "        \"top_1_pd\": topk_recall(scores_pd, label, k=1),\n",
    "        \"top_1_model\": topk_recall(scores_model, label, k=1),\n",
    "        \n",
    "        \"top_5_base\": topk_recall(scores_base, label, k=5),\n",
    "        \"top_5_pd\": topk_recall(scores_pd, label, k=5),\n",
    "        \"top_5_model\": topk_recall(scores_model, label, k=5),\n",
    "\n",
    "        \"top_10_base\": topk_recall(scores_base, label, k=10),\n",
    "        \"top_10_pd\": topk_recall(scores_pd, label, k=10),\n",
    "        \"top_10_model\": topk_recall(scores_model, label, k=10),\n",
    "\n",
    "        \"top_15_base\": topk_recall(scores_base, label, k=15),\n",
    "        \"top_15_pd\": topk_recall(scores_pd, label, k=15),\n",
    "        \"top_15_model\": topk_recall(scores_model, label, k=15),\n",
    "\n",
    "        \"top_20_base\": topk_recall(scores_base, label, k=20),\n",
    "        \"top_20_pd\": topk_recall(scores_pd, label, k=20),\n",
    "        \"top_20_model\": topk_recall(scores_model, label, k=20),\n",
    "\n",
    "        \"top_30_base\": topk_recall(scores_base, label, k=30),\n",
    "        \"top_30_pd\": topk_recall(scores_pd, label, k=30),\n",
    "        \"top_30_model\": topk_recall(scores_model, label, k=30),\n",
    "\n",
    "        \"top_40_base\": topk_recall(scores_base, label, k=40),\n",
    "        \"top_40_pd\": topk_recall(scores_pd, label, k=40),\n",
    "        \"top_40_model\": topk_recall(scores_model, label, k=40),\n",
    "\n",
    "        \"top_50_base\": topk_recall(scores_base, label, k=50),\n",
    "        \"top_50_pd\": topk_recall(scores_pd, label, k=50),\n",
    "        \"top_50_model\": topk_recall(scores_model, label, k=50),\n",
    "\n",
    "        \"top_75_base\": topk_recall(scores_base, label, k=75),\n",
    "        \"top_75_pd\": topk_recall(scores_pd, label, k=75),\n",
    "        \"top_75_model\": topk_recall(scores_model, label, k=75),\n",
    "\n",
    "        \"top_100_base\": topk_recall(scores_base, label, k=100),\n",
    "        \"top_100_pd\": topk_recall(scores_pd, label, k=100),\n",
    "        \"top_100_model\": topk_recall(scores_model, label, k=100),\n",
    "        \n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data = {f\"top{i}\":None for i in [1,5,10,15,20,30,40,50,75,100]}\n",
    "print(base_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result_df = pd.DataFrame(result)\n",
    "result_df = result_df.set_index(\"p_id\")\n",
    "result_df = result_df[list(set(result_df.columns) - {'p_id', 'scores_base', 'scores_pd', 'scores_model'})]\n",
    "\n",
    "print(result_df)\n",
    "\n",
    "data = (result_df.sum(0) / len(result_df)).to_dict()\n",
    "print(data)\n",
    "\n",
    "print(scores_base)\n",
    "\n",
    "# Initialize empty dictionaries for 'base', 'pd', and 'model' data\n",
    "# base_data = {'top1': None, 'top10': None, 'top50': None, 'top100': None}\n",
    "# pd_data = {'top1': None, 'top10': None, 'top50': None, 'top100': None}\n",
    "# model_data = {'top1': None, 'top10': None, 'top50': None, 'top100': None}\n",
    "\n",
    "base_data = {f\"top{i}\": None for i in [1,5,10,15,20,30,40,50,75,100]}\n",
    "pd_data = {f\"top{i}\": None for i in [1,5,10,15,20,30,40,50,75,100]}\n",
    "model_data = {f\"top{i}\": None for i in [1,5,10,15,20,30,40,50,75,100]}\n",
    "\n",
    "# Organize the data according to 'base', 'pd', and 'model' categories\n",
    "for key, value in data.items():\n",
    "    if 'base' in key:\n",
    "        base_data[f\"top{key.split('_')[1]}\"] = value\n",
    "    elif 'pd' in key:\n",
    "        pd_data[f\"top{key.split('_')[1]}\"] = value\n",
    "    elif 'model' in key:\n",
    "        model_data[f\"top{key.split('_')[1]}\"] = value\n",
    "\n",
    "# Create a DataFrame with 'base', 'pd', and 'model' as rows and 'top1', 'top10', 'top50', 'top100' as columns\n",
    "df = pd.DataFrame([base_data, pd_data, model_data], index=['baseline', 'Pheno2Disease', 'LLM-based'])\n",
    "df.index.name = 'Method'\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 3\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "fig_df = pd.DataFrame([base_data, pd_data, model_data], index=['Resnik-based IC', 'Pheno2Disease', 'LaRa'])\n",
    "fig_df.index.name = 'Method'\n",
    "\n",
    "\n",
    "# Example DataFrame\n",
    "# Plotting the recall curve\n",
    "plt.figure(figsize=(6, 6))\n",
    "for index, row in fig_df.iterrows():\n",
    "    plt.plot(list(row.index), list(row.values), marker='o', label=index)\n",
    "\n",
    "plt.title('Real world dataset: rare disease patient data')\n",
    "plt.xlabel('Top-k')\n",
    "plt.ylabel('Top-k Recall')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lara",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
