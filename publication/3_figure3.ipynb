{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Figure 3 - Phen2Disease 에서 썼었던 데이터셋으로 benchmark performance 확인\n",
    "- https://github.com/ZhuLab-Fudan/Phen2Disease?tab=readme-ov-file 에서 benchmark set 1\n",
    "- https://zenodo.org/records/3905420 에 위치해있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "PUBDIR = os.getcwd()\n",
    "ROOT_DIR = os.path.dirname(PUBDIR)\n",
    "DATA_DIR = os.path.join(ROOT_DIR, \"data\")\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ontology\n",
    "from core.data_model import Patient, Disease, Ontology\n",
    "from core.data_model import Diseases\n",
    "from core.io_ops import load_pickle\n",
    "\n",
    "disease_data = load_pickle(os.path.join(DATA_DIR, \"diseases.pickle\"))\n",
    "vectorized_hpo = load_pickle(os.path.join(DATA_DIR, \"hpo_definition.vector.pickle\"))\n",
    "ontology = Ontology(vectorized_hpo)\n",
    "omim_diseases = Diseases([disease for disease in disease_data if disease.id.startswith(\"OMIM\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load benchmark patient dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-30 17:22:04--  https://zenodo.org/records/3905420/files/phenopackets.zip?download=1\n",
      "Resolving zenodo.org (zenodo.org)... 188.185.79.172, 188.184.103.159, 188.184.98.238, ...\n",
      "Connecting to zenodo.org (zenodo.org)|188.185.79.172|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 572349 (559K) [application/octet-stream]\n",
      "Saving to: ‘phenopackets.zip?download=1’\n",
      "\n",
      "phenopackets.zip?do 100%[===================>] 558.93K   425KB/s    in 1.3s    \n",
      "\n",
      "2024-03-30 17:22:07 (425 KB/s) - ‘phenopackets.zip?download=1’ saved [572349/572349]\n",
      "\n",
      "Archive:  ../data/phenopackets.zip\n",
      "replace ../data/phenopackets/Naz_Villalba-2016-NLRP3-proband.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "# download public dataset from zeonodo\n",
    "! wget https://zenodo.org/records/3905420/files/phenopackets.zip?download=1\n",
    "! mv phenopackets.zip?download=1 ../data/phenopackets.zip\n",
    "! unzip -o ../data/phenopackets.zip -d ../data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phen2Disease Benchmark dataset 가져오기\n",
    "https://github.com/ZhuLab-Fudan/Phen2Disease?tab=readme-ov-file 에서 benchmark set 1\n",
    "https://zenodo.org/records/3905420 에 위치해있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from core.benchmark import load_phenopacket_patients\n",
    "from core.data_model import HPO, HPOs, Patient, Patients\n",
    "\n",
    "benchmark_patients:Patients = load_phenopacket_patients(\n",
    "    phenopacket_dir=os.path.join(DATA_DIR, \"phenopackets\"),\n",
    "    ontology=ontology\n",
    ")\n",
    "print(benchmark_patients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmark model 1: Phen2Disease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download prerequsite file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gdown\n",
    "! gdown 1CSYfDj5fG9SsosIDlG-hLAoKp9eMHxjH\n",
    "! gunzip lin_similarity_matrix.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.benchmark import get_pheno2disease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cache에서 가져오기 모든 데이터: benchmark +disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.datasets import StochasticPairwiseDataset, collate_for_stochastic_pairwise_eval\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "benchmark_dataset = StochasticPairwiseDataset(\n",
    "    benchmark_patients,\n",
    "    disease_data,\n",
    "    max_len=15,\n",
    ")\n",
    "benchmark_dataset.validate()\n",
    "\n",
    "benchmark_dataloader = DataLoader(\n",
    "    benchmark_dataset,\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_for_stochastic_pairwise_eval,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# disease cache\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from core_3asc.metric import topk_recall\n",
    "\n",
    "cached_vector = {}\n",
    "whole_disease = benchmark_dataset.disease_tensors\n",
    "with torch.no_grad():\n",
    "    for disease_id, tensor in tqdm(whole_disease.items()):\n",
    "        cached_vector[disease_id] = best_model(tensor.cuda(3)).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(benchmark_dataset)\n",
    "print(benchmark_dataloader)\n",
    "p_samples = np.random.choice(range(len(benchmark_dataloader)), 300).tolist()\n",
    "print(p_samples)\n",
    "print(disease_data[:])\n",
    "for i in benchmark_dataset:\n",
    "    print(i)\n",
    "    break\n",
    "\n",
    "for i in omim_diseases:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LaRa 가져오기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from core.datasets import (\n",
    "    StochasticPairwiseDataset,\n",
    "    collate_for_stochastic_pairwise_eval,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from core.networks import Transformer\n",
    "\n",
    "params = {\n",
    "    \"output_size\": 128,\n",
    "    \"hidden_dim\": 2048,\n",
    "    \"input_size\": 1536,\n",
    "    \"n_layers\": 32,\n",
    "    \"nhead\": 32,\n",
    "    \"batch_first\": False,\n",
    "}\n",
    "best_model = Transformer(**params)\n",
    "best_model.load_state_dict(\n",
    "    # torch.load(\n",
    "    #     \"/data/tyler_dev/working_dir/sym/symptom_similarity/data/81fcf4f39e57422db4debfeac61b01f0/val_top100_0.584.ckpt\"\n",
    "    # )\n",
    "    torch.load(\n",
    "        os.path.join(ROOT_DIR, \"data\", \"val_top100_0.584.ckpt\"),\n",
    "        map_location='cuda:3'\n",
    "    )\n",
    ")\n",
    "best_model.eval()\n",
    "best_model = best_model.cuda(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phen2Disease 모델 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 사내 증상유사도 모델 가져오기 + 계산수식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from SemanticSimilarity.calculator import NodeLevelSimilarityCalculator\n",
    "\n",
    "conf = OmegaConf.load(\"/data1/benny_dev/symptom_similarity/SemanticSimilarity/config.yaml\")\n",
    "# 원본 알고리즘 이용한 계산\n",
    "tb_cal = NodeLevelSimilarityCalculator(conf)\n",
    "tb_cal.set_level()\n",
    "tb_cal.set_mica_mat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from SemanticSimilarity.data_model import Phenotype\n",
    "\n",
    "def calculate_score(p, d):\n",
    "    node_level = {}\n",
    "    node_level[p.id] = {}\n",
    "    p_syms = {Phenotype(id_, name) for id_, name in zip(p.hpos.id2hpo.keys(), p.hpos.name2hpo.keys())}\n",
    "    d_syms = {Phenotype(id_, name) for id_, name in zip(d.hpos.id2hpo.keys(), d.hpos.name2hpo.keys())}\n",
    "    score = tb_cal.get_semantic_similarity(p_syms, d_syms)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phen2Disease 유사도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(os.path.join(DATA_DIR, \"tyler_backup/lin_similarity_matrix.json\"), \"r\") as f:\n",
    "    similarity_matrix = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 식 참고해서 계산\n",
    "# t in P, t` in D\n",
    "# s_p = sum_p(max_d(sim(t, t`))*IC(t)) / sum(IC(t))\n",
    "# s_pd = (sum_p(max_d(sim(t, t`))*IC(t)) + sum_d(max_p(sim(t, t`))*IC(t`))) /  (sum(IC(t)) + sum(IC(t`)))\n",
    "\n",
    "def get_sym(patient, disease):\n",
    "    sum_sym = 0.\n",
    "    sum_ic = 0.\n",
    "    for p_sym in patient.hpos:\n",
    "        max_sim = 0.\n",
    "        for d_sym in disease.hpos:\n",
    "            try:\n",
    "                score = similarity_matrix[p_sym.id][d_sym.id] \n",
    "            except:\n",
    "                score = 0 \n",
    "                \n",
    "            if score > max_sim:\n",
    "                max_sim = score\n",
    "\n",
    "        sum_sym += max_sim * p_sym.ic\n",
    "        sum_ic += p_sym.ic\n",
    "    \n",
    "    return sum_sym, sum_ic\n",
    "\n",
    "\n",
    "def get_pheno2disease(patient, disease):\n",
    "    sum_sym_p, sum_ic_p = get_sym(patient, disease)\n",
    "    sum_sym_d, sum_ic_d = get_sym(disease, patient)\n",
    "    sym_pd = (sum_sym_p+sum_sym_d) / (sum_ic_p + sum_ic_d)\n",
    "\n",
    "    pheno2disease = sym_pd + (sum_sym_p/sum_ic_p)\n",
    "\n",
    "    return pheno2disease\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LaRa 계산 수식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.augmentation import (\n",
    "    TruncateOrPad,\n",
    ")\n",
    "\n",
    "padder = TruncateOrPad(\n",
    "            15, stochastic=False, weighted_sampling=True\n",
    "        )\n",
    "\n",
    "def get_score_from_model(patient, disease):\n",
    "    with torch.no_grad():\n",
    "        input_src = padder(\n",
    "            torch.tensor(patient.hpos.vector, dtype=torch.float32, device=\"cuda:3\"), patient\n",
    "        )\n",
    "        target_src = padder(\n",
    "            torch.tensor(disease.hpos.vector, dtype=torch.float32, device=\"cuda:3\"), disease\n",
    "        )\n",
    "\n",
    "        input_vector = best_model(input_src)\n",
    "        target_vector = best_model(target_src)\n",
    "        # target_vector = cached_vector[disease.id]\n",
    "\n",
    "        scores = (\n",
    "            torch.nn.functional.cosine_similarity(\n",
    "                input_vector, target_vector\n",
    "            )\n",
    "            .squeeze(-1)\n",
    "            .detach()\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "\n",
    "        del input_src\n",
    "        del input_vector\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return scores\n",
    "    \n",
    "\n",
    "def get_att_weight(disease):\n",
    "    with torch.no_grad():\n",
    "        input_src = padder(\n",
    "            torch.tensor(disease.hpos.vector, dtype=torch.float32).cuda(3), disease\n",
    "        )\n",
    "        return best_model.get_att_weight(input_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in disease_data_sample:\n",
    "    print(i.vector)\n",
    "    break\n",
    "\n",
    "# for i in benchmark_patients:\n",
    "#     print(i)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 비교 평가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from core_3asc.metric import topk_recall\n",
    "\n",
    "disease_data_sample = [i for i in omim_diseases]\n",
    "patient_data_sample = [i for i in benchmark_patients]\n",
    "\n",
    "result = []\n",
    "for p in tqdm(patient_data_sample):\n",
    "\n",
    "    label = np.zeros((len(disease_data_sample), ))\n",
    "    scores_base = np.zeros((len(disease_data_sample)),)\n",
    "    scores_pd = np.zeros((len(disease_data_sample)), )\n",
    "    scores_model = np.zeros((len(disease_data_sample)), )\n",
    "\n",
    "    # p_vector = best_model(\n",
    "    #     torch.from_numpy(patient.hpos.vector).cuda(3).float()\n",
    "    # ).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "    for i, d in enumerate(disease_data_sample):\n",
    "        if d.id in p.disease_ids:\n",
    "            label[i] = 1\n",
    "\n",
    "        scores_base[i] = calculate_score(p, d)\n",
    "        scores_pd[i] = get_pheno2disease(p, d)\n",
    "        scores_model[i] = get_score_from_model(p, d)\n",
    "    \n",
    "    result.append({\n",
    "        \"p_id\": p.id, \n",
    "        \"scores_base\": scores_base, \n",
    "        \"scores_pd\": scores_pd, \n",
    "        \"scores_model\": scores_model, \n",
    "\n",
    "        \"top_1_base\": topk_recall(scores_base, label, k=1),\n",
    "        \"top_1_pd\": topk_recall(scores_pd, label, k=1),\n",
    "        \"top_1_model\": topk_recall(scores_model, label, k=1),\n",
    "        \n",
    "        \"top_5_base\": topk_recall(scores_base, label, k=5),\n",
    "        \"top_5_pd\": topk_recall(scores_pd, label, k=5),\n",
    "        \"top_5_model\": topk_recall(scores_model, label, k=5),\n",
    "\n",
    "        \"top_10_base\": topk_recall(scores_base, label, k=10),\n",
    "        \"top_10_pd\": topk_recall(scores_pd, label, k=10),\n",
    "        \"top_10_model\": topk_recall(scores_model, label, k=10),\n",
    "\n",
    "        \"top_15_base\": topk_recall(scores_base, label, k=15),\n",
    "        \"top_15_pd\": topk_recall(scores_pd, label, k=15),\n",
    "        \"top_15_model\": topk_recall(scores_model, label, k=15),\n",
    "\n",
    "        \"top_20_base\": topk_recall(scores_base, label, k=20),\n",
    "        \"top_20_pd\": topk_recall(scores_pd, label, k=20),\n",
    "        \"top_20_model\": topk_recall(scores_model, label, k=20),\n",
    "\n",
    "        \"top_30_base\": topk_recall(scores_base, label, k=30),\n",
    "        \"top_30_pd\": topk_recall(scores_pd, label, k=30),\n",
    "        \"top_30_model\": topk_recall(scores_model, label, k=30),\n",
    "\n",
    "        \"top_40_base\": topk_recall(scores_base, label, k=40),\n",
    "        \"top_40_pd\": topk_recall(scores_pd, label, k=40),\n",
    "        \"top_40_model\": topk_recall(scores_model, label, k=40),\n",
    "\n",
    "        \"top_50_base\": topk_recall(scores_base, label, k=50),\n",
    "        \"top_50_pd\": topk_recall(scores_pd, label, k=50),\n",
    "        \"top_50_model\": topk_recall(scores_model, label, k=50),\n",
    "\n",
    "        \"top_75_base\": topk_recall(scores_base, label, k=75),\n",
    "        \"top_75_pd\": topk_recall(scores_pd, label, k=75),\n",
    "        \"top_75_model\": topk_recall(scores_model, label, k=75),\n",
    "\n",
    "        \"top_100_base\": topk_recall(scores_base, label, k=100),\n",
    "        \"top_100_pd\": topk_recall(scores_pd, label, k=100),\n",
    "        \"top_100_model\": topk_recall(scores_model, label, k=100),\n",
    "        \n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data = {f\"top{i}\":None for i in [1,5,10,15,20,30,40,50,75,100]}\n",
    "print(base_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result_df = pd.DataFrame(result)\n",
    "result_df = result_df.set_index(\"p_id\")\n",
    "result_df = result_df[list(set(result_df.columns) - {'p_id', 'scores_base', 'scores_pd', 'scores_model'})]\n",
    "\n",
    "print(result_df)\n",
    "\n",
    "data = (result_df.sum(0) / len(result_df)).to_dict()\n",
    "print(data)\n",
    "\n",
    "print(scores_base)\n",
    "\n",
    "# Initialize empty dictionaries for 'base', 'pd', and 'model' data\n",
    "# base_data = {'top1': None, 'top10': None, 'top50': None, 'top100': None}\n",
    "# pd_data = {'top1': None, 'top10': None, 'top50': None, 'top100': None}\n",
    "# model_data = {'top1': None, 'top10': None, 'top50': None, 'top100': None}\n",
    "\n",
    "base_data = {f\"top{i}\": None for i in [1,5,10,15,20,30,40,50,75,100]}\n",
    "pd_data = {f\"top{i}\": None for i in [1,5,10,15,20,30,40,50,75,100]}\n",
    "model_data = {f\"top{i}\": None for i in [1,5,10,15,20,30,40,50,75,100]}\n",
    "\n",
    "# Organize the data according to 'base', 'pd', and 'model' categories\n",
    "for key, value in data.items():\n",
    "    if 'base' in key:\n",
    "        base_data[f\"top{key.split('_')[1]}\"] = value\n",
    "    elif 'pd' in key:\n",
    "        pd_data[f\"top{key.split('_')[1]}\"] = value\n",
    "    elif 'model' in key:\n",
    "        model_data[f\"top{key.split('_')[1]}\"] = value\n",
    "\n",
    "# Create a DataFrame with 'base', 'pd', and 'model' as rows and 'top1', 'top10', 'top50', 'top100' as columns\n",
    "df = pd.DataFrame([base_data, pd_data, model_data], index=['baseline', 'Pheno2Disease', 'LLM-based'])\n",
    "df.index.name = 'Method'\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 3\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "fig_df = pd.DataFrame([base_data, pd_data, model_data], index=['Resnik-based IC', 'Pheno2Disease', 'LaRa'])\n",
    "fig_df.index.name = 'Method'\n",
    "\n",
    "\n",
    "# Example DataFrame\n",
    "# Plotting the recall curve\n",
    "plt.figure(figsize=(6, 6))\n",
    "for index, row in fig_df.iterrows():\n",
    "    plt.plot(list(row.index), list(row.values), marker='o', label=index)\n",
    "\n",
    "plt.title('Real world dataset: rare disease patient data')\n",
    "plt.xlabel('Top-k')\n",
    "plt.ylabel('Top-k Recall')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lara",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
